{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiuLhv1PF5jjsXidwjGfNp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParasKanchan/Material_Science_Project/blob/main/Material_Science_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KjfRxjHxM3I-"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "MLP_HIDDEN = [128, 64]\n",
        "DROPOUT = 0.2\n",
        "LR = 1e-3\n",
        "EPOCHS = 80\n",
        "PATIENCE = 10\n",
        "TARGET = \"creep_time_h\"\n",
        "\n",
        "def load_and_prepare(csv_path: str, drop_cols: List[str] = None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
        "    if drop_cols:\n",
        "        for c in drop_cols:\n",
        "            if c in df.columns:\n",
        "                df = df.drop(columns=[c])\n",
        "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    cat_cols = [c for c in cat_cols if c != TARGET]\n",
        "    for c in cat_cols:\n",
        "        df[c] = df[c].astype(\"category\").cat.codes\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [c for c in numeric_cols if c != TARGET]\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "    X = df[numeric_cols].values\n",
        "    y = df[TARGET].values.astype(float)\n",
        "    y = np.log1p(y)\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "    return Xs, y, numeric_cols, scaler\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_in, hidden, p):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = n_in\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(p))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(model, tr_loader, val_loader, epochs, lr, patience):\n",
        "    model.to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    best_loss = 1e9\n",
        "    best_state = None\n",
        "    no_imp = 0\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in tr_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            loss = loss_fn(model(xb), yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                val_losses.append(loss_fn(model(xb), yb).item())\n",
        "        val_loss = float(np.mean(val_losses))\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = model.state_dict()\n",
        "            no_imp = 0\n",
        "        else:\n",
        "            no_imp += 1\n",
        "            if no_imp >= patience:\n",
        "                break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Iryr7aL9OM-B"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true_log, y_pred_log):\n",
        "    y_true = np.expm1(y_true_log)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
        "    mae_raw = mean_absolute_error(y_true, y_pred)\n",
        "    rel = mae_raw / np.median(y_true)\n",
        "    return {\"rmse_log\": rmse_log, \"mae_raw\": mae_raw, \"rel_mae_med\": rel}"
      ],
      "metadata": {
        "id": "D3NJjWO9OY3y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgb(X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"max_depth\": 4,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"n_estimators\": 500,\n",
        "        \"random_state\": SEED,\n",
        "        \"verbosity\": 0,\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "\n",
        "    try:\n",
        "        early_stop_cb = xgb.callback.EarlyStopping(rounds=50, save_best=True)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[early_stop_cb], verbose=False)\n",
        "        return model\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
        "        return model\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.fit(X_train, y_train, verbose=False)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "sJ6LNRfiRXCR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    try:\n",
        "        if not os.path.exists(args.data):\n",
        "            raise FileNotFoundError(f\"Data file not found: {args.data}\")\n",
        "        X, y, features, scaler = load_and_prepare(args.data)\n",
        "        print(\"DATA SHAPE:\", \"X\", X.shape, \"y\", y.shape, \"features_count\", len(features))\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "        print(\"SPLITS:\", X_tr.shape, X_val.shape if 'X_val' in globals() else \"no X_val yet\")\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.15, random_state=SEED)\n",
        "        print(\"TRAIN/VAL shapes:\", X_train.shape, X_val.shape)\n",
        "        xgb_model = train_xgb(X_train, y_train, X_val, y_val)\n",
        "        xgb_pred = xgb_model.predict(X_te)\n",
        "        print(\"XGBoost:\", evaluate(y_te, xgb_pred))\n",
        "        ds_tr = TabularDataset(X_train, y_train)\n",
        "        ds_val = TabularDataset(X_val, y_val)\n",
        "        ds_te = TabularDataset(X_te, y_te)\n",
        "        dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        model = MLP(n_in=X.shape[1], hidden=MLP_HIDDEN, p=DROPOUT)\n",
        "        model = train_mlp(model, dl_tr, dl_val, args.epochs, args.lr, args.patience)\n",
        "        preds_list = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in dl_te:\n",
        "                xb = xb.to(DEVICE)\n",
        "                out = model(xb).cpu().numpy().ravel()\n",
        "                preds_list.append(out)\n",
        "        if len(preds_list) == 0:\n",
        "            raise RuntimeError(\"No predictions produced: test DataLoader is empty.\")\n",
        "        preds = np.concatenate(preds_list)\n",
        "        if preds.shape[0] != y_te.shape[0]:\n",
        "            print(\"WARNING: prediction count\", preds.shape[0], \"!= test target count\", y_te.shape[0])\n",
        "            # try to align by trimming/padding if necessary\n",
        "            min_len = min(preds.shape[0], y_te.shape[0])\n",
        "            preds = preds[:min_len]\n",
        "            y_te = y_te[:min_len]\n",
        "        print(\"MLP:\", evaluate(y_te, preds))\n",
        "        os.makedirs(args.out_dir, exist_ok=True)\n",
        "        xgb_model.save_model(os.path.join(args.out_dir, \"xgb.json\"))\n",
        "        torch.save(model.state_dict(), os.path.join(args.out_dir, \"mlp.pt\"))\n",
        "        import joblib\n",
        "        joblib.dump(scaler, os.path.join(args.out_dir, \"scaler.joblib\"))\n",
        "        joblib.dump(features, os.path.join(args.out_dir, \"features.joblib\"))\n",
        "        print(\"Saved models and artifacts to:\", args.out_dir)\n",
        "    except Exception as e:\n",
        "        import traceback, sys\n",
        "        print(\"ERROR in main():\", str(e))\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--data\", required=True)\n",
        "    p.add_argument(\"--out_dir\", default=\"models\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=EPOCHS)\n",
        "    p.add_argument(\"--lr\", type=float, default=LR)\n",
        "    p.add_argument(\"--patience\", type=int, default=PATIENCE)\n",
        "    if 'IPython' in sys.modules:\n",
        "        args = p.parse_args(args=[\"--data\", \"realistic_synthetic_superalloy_dataset.csv\", \"--out_dir\", \"models\"])\n",
        "    else:\n",
        "        args = p.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvlMh_4VQ9qs",
        "outputId": "9231ca2f-d77c-4cd7-8b39-c19ed4111fb5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA SHAPE: X (1200, 27) y (1200,) features_count 27\n",
            "SPLITS: (960, 27) no X_val yet\n",
            "TRAIN/VAL shapes: (816, 27) (144, 27)\n",
            "XGBoost: {'rmse_log': np.float64(0.39832620132147456), 'mae_raw': 39.76926771589915, 'rel_mae_med': np.float64(0.8250371909611257)}\n",
            "MLP: {'rmse_log': np.float64(0.5430716893051184), 'mae_raw': 52.76254599315325, 'rel_mae_med': np.float64(1.0945905025237692)}\n",
            "Saved models and artifacts to: models\n"
          ]
        }
      ]
    }
  ]
}